Hello. I'm Maekawa Toshiki.
I'm researching on natural language processing and understanding in Chandrasekhar Project.
Chandrasekhar Project is one of the science clubs in my school.
Today I'll tell you my research.
First, look at this sentence. 'HOW WILL THE WEATHER BE'
OK? If you are human, you are able to understand what this sentence means.
But this is a difficult problem for computers.
Because this sentence is just a string, so it has no meaning for computers.
These years, computers have been developing and getting great results even ordinary people know in the feild of image recognition or voice recognition.
Self-driving cars or voice input are good examples.
How about the field of 'natural language'? Do you know AI assistants such as Siri or Google Assistant?
If you've ever used it, you know that they give up understanding meanings of a bit hard questions.
Then I thought I couldn't do something.
This was exactly the chance I began to research on making computers abstract meanings of sentences.
OK, now I'll explain how it works.
First, ambiguous grammar makes it difficult for comuters to abstract the meaning of sentences in natural language.
Both of programming languages for computers and natural language used by human have each grammar.
But there is an absolute difference. That is, as I said, ambiguousness.
Understanding languages containing ambiguousness is not easy for even human. But in most cases, it doesn't matter.
It has not been clarified why human can use them.
But the existence of grammar in natural language means that computers should be able to deal with them.
Indeed, these years, the accuracy of machine translation services has been improving.
If you've not used them, don't you give them a try? You'll feel like that they are more useful than you thought.
Then, how can we make computers be able to abstract the meanings?
These years, AI as we say has been developing because of the technology called "Deep Learning".
Before explaining Deep Learning, I first explain Neural Network.
Neural Network is a technology that can approach to any arbitrary functions by imitating the neural circuits of our brain.
Especially in Hierarchical Neural Network, its internal structure is devided as input layer, hidden layer and output layer.
If you have good intuition, maybe you can figure out the 'Deep' of 'Deep Learning' means that there're plural hidden layers.
Plural hidden layers usually make it so hard to train computers because of enormous amounts of calculation that Deep Learning was less likely to be used.
But the improvements of computers has made Deep Learning a greatly useful technology.
After that, AI, that now can deal with high level tasks, has been developing.
This time, the Recurrent Neural Network I used in this research is also one of the Deep Learning technologies.
Recurrent Neural Network can deal with time-series data.
// already provided background knowledge?
And I used Seq-to-Seq Model, one of the Recurrent Neural Network, to train a computer.
This model can output variable length data from variable length input data.
And it's famous for being used in machine translation these years.
But this model has a problem, which is so bad at training of long inputs.
In order to make up for the problem, Attention Mechanism is used.
This is to train each correspondence between input and output.
Herewith, the accuracy of training improves.
And I had the model given input as sentences in natural language and output as the meaning data in formal language train.
