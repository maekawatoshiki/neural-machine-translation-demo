Hello. I'm Maekawa Toshiki.
I'm a member of Chandrasekhar Project.
Chandrasekhar Project is one of the science clubs in my school.
I'm doing research on natural language processing and understanding.
First, please look at this sentence. 'HOW WILL THE WEATHER BE?'
OK? If you are human, you are able to understand what this sentence means.
But this is a difficult problem for computers.
Because this sentence is just a string of words, so it has no meaning for computers.
These years, computers have been developing.
And getting great results even ordinary people know. 
For example, self-driving cars or AI assistant.
How about the field of 'natural language'? Do you know AI Siri or Google Assistant?
If you've ever used it, you know that they give up understanding meanings of complicated questions.
Then I thought I could do something.
This is why I began to do a research on making computers abstract meanings of sentences.
OK, now I'll explain how it works.
First, ambiguous grammar in natural languege makes it difficult for comuters to abstract the meaning of sentences.
Natural languages used by humans have grammer.
So does programming languages for computers.
But there is an absolute difference. That is, as I said, ambiguousness.
Understanding ambiguous languages is not easy for even human. But in most cases, it doesn't matter.
It has not been clarified why humans can use ambiguous languages.
But the existence of grammar in natural language means that computers should be able to deal with them.
Indeed, these years, the accuracy of machine translation services has been improving.
If you've not used them, won't you give them a try? You would feel they are more useful than you thought.
Then, how can we make computers be able to abstract the meanings?
These years, AI has been developing thanks to the technology called "Deep Learning".
Before explaining Deep Learning, let me explain Neural Network.
Neural Network is a technology that can approach to any arbitrary functions by imitating the neural circuits of our brain.
Especially in Multilayer Neural Network, its internal structure is devided into 3 layers.
If you have good intuition, maybe you can figure out the 'Deep' in 'Deep Learning' means that there're multiple hidden layers.
Multiple hidden layers need enourmous amount of calculation.
And usually make it so hard to train computers.
So deep Learning was less likely to be used.
But the improvements of computers has made Deep Learning a greatly useful technology.
After that, AI has been developing.
This time, the Recurrent Neural Network I used in this research is also one of the Deep Learning technologies.
Recurrent Neural Network can deal with time-series data.
And I used Seq-to-Seq Model, one of the Recurrent Neural Network, to train a computer.
And it's often used for machine translation these years.
But this model has a problem.
It is so bad at training of long inputs.
In order to make up for the problem, Attention Mechanism is used.
This is to train each correspondence between input and output.
Herewith, the accuracy of learning improves.
I trained a computer with the model.
And the model was given input as sentences in natural language, and output as the meaning data in formal language.
I trained the computer with sentences containing what you most ask to AI assistant.
For example, specific contents like "How is the weather?" or "tell me my plans tomorrow."
But I couldn't prepare so many sentences.
Because I had to make meaning lists of sentences manually.
For that reason, this time about 40 sentences are used to train the computer.
You probably thought that it's too few. You right, it may not be enough.
Preparing so many sentences that fits to my purpose is a difficult thing. In addition, the computer takes a long time to train.
These problems are important issues for the next research.
OK, I'll explain what has become.
I show some sentences I let the trained computer recognize.
Of course though there are more succeeded examples and failed examples to recognize, I thought they are good examples and chose them.
As they show, sentences contained in dataset such as 'how will the weather be' are , of course, recognized correctly.
Some sentences not contained in dataset are also recognized correctly.
They shows that the computer is traind nicely.
On the other hand, there are some samples not recognized correctly.
'Can you tell me how the weather is' and so on.
But 'tell me how the weather will be' is recognized correctly.
What I want to say is, first, 'can you tell me how the weather is' and 'tell me how the weather will be' have 'tell me how the weather' in common.
And the common part has the most part of meaning of sentence.
That means the computer couldn't see which part is concentrated the meaning of sentence.
If I could prepare more sample sentences, probably they make the accuracy of understanding of meanings better.
And I think there's one more problem.
Humans, especially children, come to be able to learn languages without bothering to tell the meanings.
However, I bothered to prepare lists of meanings to train the computer this time.
Common sense tells that it's distinctly inefficient.
Although there is no help unless having been clarified the mechanism of humans' languages,
in order for computers to really understand languages, a drastic change of technology is required.
